from gym.core import Env
import numpy as np
import random
from envs.config_SimPy import *
from gym import spaces
import numpy as np
from envs.config_RL import * 
# import environment as env # for Meta-learning
import envs.environment as env #for baseline_test
from envs.log_SimPy import *
from envs.log_RL import *
import envs.scenarios as scenarios

class MetaEnv(Env):
    """
    Wrapper around OpenAI gym environments, interface for meta learning
    """
    def __init__(self):
        
        #print("Tensorboard Directory: :", TENSORFLOW_LOGS)
        super(MetaEnv, self).__init__()

        self.scenario = None

        self.shortages = 0
        self.total_reward = 0

        # Record the cumulative value of each cost
        self.cost_dict = {
            'Holding cost': 0,
            'Process cost': 0,
            'Delivery cost': 0,
            'Order cost': 0,
            'Shortage cost': 0
        }
        
        os = []
        # Define action space
        self.action_space = spaces.Box(low = 0, high = 10, shape = (MAT_COUNT, 1), dtype = int)
        # if self.scenario["Dist_Type"] == "UNIFORM":
        #    k = INVEN_LEVEL_MAX*2+(self.scenario["max"]+1)

        # DAILY_CHANGE + INTRANSIT + REMAINING_DEMAND
        os = spaces.Box(low = 0, high = INVEN_LEVEL_MAX*2+1, shape=(len(I[ASSEMBLY_PROCESS])*(1+DAILY_CHANGE)+MAT_COUNT*INTRANSIT+1,1), dtype=int)
        self.candidate_tasks = None
        '''
        - Inventory Level of Product
        - Daily Change of Product
        - Inventory Level of WIP
        - Daily Change of WIP
        - Inventory Level of Material
        - Daily Change of Material
        - Demand - Inventory Level of Product
        '''
        self.observation_space = os
        self.task = None

    def create_scenarios(self):
        return scenarios.create_scenarios()
    
    def sample_tasks(self, n_tasks):
        """
        Samples task of the meta-environment

        Args:
            n_tasks (int) : number of different meta-tasks needed

        Returns:
            tasks (list) : an (n_tasks) length list of tasks
        """
        # üîπ Ïù¥ÎØ∏ Ïô∏Î∂ÄÏóêÏÑú env.set_task(...)Î°ú ÏãúÎÇòÎ¶¨Ïò§Í∞Ä Ï†ïÌï¥Ï†∏ ÏûàÏúºÎ©¥
        #     -> Í∑∏ ÏãúÎÇòÎ¶¨Ïò§Îßå Í≥ÑÏÜç Ïì∞ÎèÑÎ°ù ÌïúÎã§.
        if self.scenario is not None:
            # num_tasks=1Ïù¥Î©¥ [self.scenario] ÌïòÎÇòÎßå Îì§Ïñ¥Í∞Ñ Î¶¨Ïä§Ìä∏Í∞Ä ÎÑòÏñ¥Í∞ê
            self.tasks = [self.scenario for _ in range(n_tasks)]
            return self.tasks

        # üîπ meta-training(ÏõêÎûò ProMP ÏÑ∏ÌåÖ)Ï≤òÎüº Ïó¨Îü¨ taskÎ•º ÎΩëÍ≥† Ïã∂ÏùÑ ÎïåÎäî
        #     env.set_task()Î•º Ïïà Î∂ÄÎ•¥Î©¥ Í∏∞Ï°¥ Î°úÏßÅÏù¥ Ïã§ÌñâÎê®
        print("all tasks reset")
        self.candidate_tasks = self.create_scenarios()
        self.tasks = random.sample(self.candidate_tasks, n_tasks)
        return self.tasks

    def set_task(self, task):
        #print("Task_Setted: ", task)
        """
        Sets the specified task to the current environment

        Args:
            task: task of the meta-learning environment
        """
        self.scenario = task

    def get_task(self):
        """
        Gets the task that the agent is performing in the current environment

        Returns:
            task: task of the meta-learning environment
        """
        return self.scenario

    def reset(self):
        # Initialize the total reward for the episode
        self.cost_dict = {
            'Holding cost': 0,
            'Process cost': 0,
            'Delivery cost': 0,
            'Order cost': 0,
            'Shortage cost': 0
        }
        # Initialize the simulation environment
        self.simpy_env, self.inventoryList, self.procurementList, self.productionList, self.sales, self.customer, self.providerList, self.daily_events = env.create_env(
            I, P, DAILY_EVENTS)
        env.simpy_event_processes(self.simpy_env, self.inventoryList, self.procurementList,
                                  self.productionList, self.sales, self.customer, self.providerList, self.daily_events, I, self.scenario)
        env.update_daily_report(self.inventoryList)

        state_real = self.get_current_state()
        STATE_DICT.clear()
        DAILY_REPORTS.clear()

        return state_real

    def step(self, action):

        # Update the action of the agent
        test_actions = []
        # Update the action of the agent
        if RL_ALGORITHM == "PPO":
            i = 0
            for _ in range(len(I[ASSEMBLY_PROCESS])):
                if I[ASSEMBLY_PROCESS][_]["TYPE"] == "Material":
                    # Set action as predicted value
                    I[ASSEMBLY_PROCESS][_]["LOT_SIZE_ORDER"] = min(max(np.round(action[i]),0),10) # ÏñëÏàò ÏÉÅÌïú
                    i += 1
                    test_actions.append(I[ASSEMBLY_PROCESS][_]['LOT_SIZE_ORDER'])

        elif RL_ALGORITHM == "DQN":
            pass

        # Capture the current state of the environment
        # current_state = env.cap_current_state(self.inventoryList)
        # Run the simulation for 24 hours (until the next day)
        # Action append

        self.simpy_env.run(until=self.simpy_env.now + 24)
        env.update_daily_report(self.inventoryList)
        # Capture the next state of the environment
        state_real = self.get_current_state()
        # Set the next state
        next_state = state_real
        # Calculate the total cost of the day
        cost = env.Cost.update_cost_log(self.inventoryList)
        # Cost Dict update
        for key in DAILY_COST_REPORT.keys():
            self.cost_dict[key] += DAILY_COST_REPORT[key]

        env.Cost.clear_cost()
        reward = -cost
        self.total_reward += reward
        self.shortages += self.sales.num_shortages
        self.sales.num_shortages = 0

        if PRINT_SIM:
            # Print the simulation log every 24 hours (1 day)
            print(f"\nDay {(self.simpy_env.now+1) // 24}:")
            if RL_ALGORITHM == "PPO":
                i = 0
                for _ in range(len(I)):
                    if I[ASSEMBLY_PROCESS][_]["TYPE"] == "Raw Material":
                        print(
                            f"[Order Quantity for {I[ASSEMBLY_PROCESS][_]['NAME']}] ", action[i])
                        i += 1
            # SimPy simulation print
            for log in self.daily_events:
                print(log)
            print("[Daily Total Cost] ", -reward)
            print("Total cost: ", -self.total_reward)
            print("[REAL_STATE for the next round] ",  [
                    item for item in next_state])

        self.daily_events.clear()

        # Check if the simulation is done
        done = self.simpy_env.now >= SIM_TIME * 24  # Ïòà: SIM_TIMEÏùº Ïù¥ÌõÑÏóê Ï¢ÖÎ£å

        info = {}  # Ï∂îÍ∞Ä Ï†ïÎ≥¥ (ÌïÑÏöîÏóê Îî∞Îùº ÏÇ¨Ïö©)

        return next_state, reward, done, self.cost_dict
    
    def get_current_state(self):
        # Make State for RL
        state = []
        # Update STATE_ACTION_REPORT_REAL
        for id in range(len(I[ASSEMBLY_PROCESS])):
            # ID means Item_ID, 7 means to the length of the report for one item
            # append On_Hand_inventory
            state.append(
                STATE_DICT[-1][f"On_Hand_{I[ASSEMBLY_PROCESS][id]['NAME']}"]+INVEN_LEVEL_MAX)
            # append changes in inventory
            if DAILY_CHANGE == 1:
                # append changes in inventory
                state.append(
                    STATE_DICT[-1][f"Daily_Change_{I[ASSEMBLY_PROCESS][id]['NAME']}"]+INVEN_LEVEL_MAX)
            if INTRANSIT == 1:
                if I[ASSEMBLY_PROCESS][id]["TYPE"] == "Material":
                    # append Intransition inventory
                    state.append(
                        STATE_DICT[-1][f"In_Transit_{I[ASSEMBLY_PROCESS][id]['NAME']}"])
        '''
        # Append remaining demand
        print(I[ASSEMBLY_PROCESS][0]["DEMAND_QUANTITY"] -
                     self.inventoryList[0].on_hand_inventory+INVEN_LEVEL_MAX)
        '''
        state.append(I[ASSEMBLY_PROCESS][0]["DEMAND_QUANTITY"] -
                     self.inventoryList[0].on_hand_inventory+INVEN_LEVEL_MAX)
        
        return state
    def report_scalar(self, paths, rollouts):
        """
        Logs env-specific diagnostic information

        Args:
            paths (list) : list of all paths collected with this env during this iteration
            prefix (str) : prefix for logger
        """
        infos = {}

        rewards = 0
        for path in paths:
            rewards += sum(path['rewards'])

        average = rewards/(len(paths)*rollouts)
        
        cost_dict = {
            'Holding cost': 0,
            'Process cost': 0,
            'Delivery cost': 0,
            'Order cost': 0,
            'Shortage cost': 0
        }
        
        for key in self.cost_dict.keys():
            for path in paths:
                cost_dict[key] = cost_dict[key] + path['env_infos'][key][-1]
                
        infos['reward'] = average
        infos['cost'] = cost_dict
        return infos
 
    