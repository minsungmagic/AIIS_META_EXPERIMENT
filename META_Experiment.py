import numpy as np
from envs.config_SimPy import *
from envs.promp_env import MetaEnv
from AIIS_META.Agents.Gaussian.Meta_Gaussian import MetaGaussianAgent
from AIIS_META.Baselines.linear_baseline import LinearFeatureBaseline
from AIIS_META.Algos.MAML.promp import ProMP
from AIIS_META.Agents.Simple_Mlp import SimpleMLP
import torch
import torch.optim as optim
from envs.config_folders import *


import pandas as pd
import os

from envs.scenarios import get_scenarios
from collections import OrderedDict

class ScenarioEvalCallback:
    """
    ê° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í•™ìŠµì´ ëë‚œ ë’¤ reward_historyë¥¼ ë°›ì•„ì„œ
    ë§ˆì§€ë§‰ windowê°œ reward í‰ê· ì„ results ë¦¬ìŠ¤íŠ¸ì— í•œ ì¤„ë¡œ ì¶”ê°€í•˜ê³ ,
    ì‹œë‚˜ë¦¬ì˜¤ë³„ ìš”ì•½ ì •ë³´(ë¶„í¬, demand, leadtime, avg_reward)ë¥¼ ì—‘ì…€ë¡œ ì €ì¥.
    """
    def __init__(self, window, scenario_idx, scenario, results_list):
        self.window = window
        self.scenario_idx = scenario_idx
        self.scenario = scenario
        self.results_list = results_list

    def __call__(self, reward_history):
        print(f"[Scenario {self.scenario_idx}] reward_history length = {len(reward_history)}")

        if len(reward_history) < self.window:
            print(f"[Scenario {self.scenario_idx}] reward ê°œìˆ˜ê°€ {self.window}ê°œë³´ë‹¤ ì ì–´ì„œ í‰ê·  ê³„ì‚° ìƒëµ")
            return

        # ë§ˆì§€ë§‰ windowê°œ rewardë¡œ í‰ê·  ê³„ì‚°
        last_rewards = reward_history[-self.window:]
        mean_reward = float(np.mean(last_rewards))

        print(f"[Scenario {self.scenario_idx}] ë§ˆì§€ë§‰ {self.window}ê°œ reward í‰ê· : {mean_reward:.3f}")

        # âœ… ìµœì¢… ê²°ê³¼ìš© ë¦¬ìŠ¤íŠ¸ì—ëŠ” ë„¤ ê°œ í•„ë“œë§Œ ì €ì¥
        self.results_list.append({
            "scenario_type": self.scenario["Scenario"],   # ë¶„í¬ íƒ€ì… (Gaussian / Uniform / Complex ë“±)
            "demand": str(self.scenario["DEMAND"]),       # demand ì„¤ì • (dictë¥¼ ë¬¸ìì—´ë¡œ)
            "leadtime": str(self.scenario["LEADTIME"]),   # leadtime ì„¤ì • (dictë¥¼ ë¬¸ìì—´ë¡œ)
            "avg_reward": mean_reward,                    # ë§ˆì§€ë§‰ window í‰ê· 
        })

        # ================================
        # ğŸ”¥ ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½ ì—‘ì…€ ì €ì¥ ë¶€ë¶„
        # ================================
        summary_df = pd.DataFrame([{
            "scenario_type": self.scenario["Scenario"],   # ë¶„í¬ íƒ€ì…
            "demand": str(self.scenario["DEMAND"]),
            "leadtime": str(self.scenario["LEADTIME"]),
            "avg_reward": mean_reward,                    # ë§ˆì§€ë§‰ window í‰ê· 
        }])

        save_path = os.path.join(
            SAVED_MODEL_PATH,
            f"scenario_{self.scenario_idx}_summary.xlsx"
        )
        summary_df.to_excel(save_path, index=False)
        print(f"[Scenario {self.scenario_idx}] ìš”ì•½ Excel ì €ì¥ ì™„ë£Œ â†’ {save_path}")





class EvalCSVCallback:
    """
    í•™ìŠµì´ ëë‚œ ë’¤ reward_history(ì—í­ë³„ reward ë¦¬ìŠ¤íŠ¸)ë¥¼ ë°›ì•„ì„œ
    ë§ˆì§€ë§‰ windowê°œ í‰ê· ì„ ê³„ì‚°í•˜ê³  CSVë¡œ ì €ì¥í•˜ëŠ” ì½œë°±.
    """
    def __init__(self, window: int = 15, csv_path: str = None):
        self.window = window
        # ê¸°ë³¸ ê²½ë¡œ: SAVED_MODEL_PATH / eval_rewards.csv
        if csv_path is None:
            self.csv_path = os.path.join(SAVED_MODEL_PATH, "eval_rewards.csv")
        else:
            self.csv_path = csv_path

    def __call__(self, reward_history):
        print("========== reward_history ==========")
        print(reward_history)

        if len(reward_history) < self.window:
            print(f"[EvalCSVCallback] reward ê°œìˆ˜ê°€ {self.window}ê°œë³´ë‹¤ ì ì–´ì„œ í‰ê· ì„ ë‚¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë§ˆì§€ë§‰ windowê°œì˜ reward ì‚¬ìš©
        last_rewards = reward_history[-self.window:]
        mean_reward = float(np.mean(last_rewards))

        print(f"[EvalCSVCallback] ë§ˆì§€ë§‰ {self.window}ê°œ reward í‰ê· : {mean_reward:.3f}")
        print(f"[EvalCSVCallback] CSV ì €ì¥ ê²½ë¡œ: {self.csv_path}")

        # CSVë¡œ ì €ì¥
        df = pd.DataFrame({
            "mean_reward_last_window": [mean_reward],
            "num_rewards": [len(last_rewards)],
        })

        # í•œ ë²ˆë§Œ ì“°ë©´ ë˜ë‹ˆê¹Œ í•­ìƒ ë®ì–´ì“°ê¸°(mode='w')
        df.to_csv(self.csv_path, index=False)

def main(params):

    # 0) í™˜ê²½ê³¼ ì‹œë‚˜ë¦¬ì˜¤ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
    env = MetaEnv()
    scenario_list = get_scenarios()  # ë„¤ê°€ ì •ì˜í•œ ì‹œë‚˜ë¦¬ì˜¤ë“¤

    # ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    all_results = []

    # ===== ì‹œë‚˜ë¦¬ì˜¤ë³„ ë£¨í”„ =====
    for idx, scenario in enumerate(scenario_list):
        print("\n" + "=" * 80)
        print(f"[{idx+1}/{len(scenario_list)}] Scenario Fine-tuning Start")
        print("Scenario:", scenario)
        print("=" * 80)

        # 1) í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í™˜ê²½ì— ì„¸íŒ…
        env.set_task(scenario)

        # 2) ë„¤íŠ¸ì›Œí¬ / ì—ì´ì „íŠ¸ / ì•Œê³ ë¦¬ì¦˜ ìƒˆë¡œ ë§Œë“¤ê¸°
        mlp = SimpleMLP(
            np.prod(env.observation_space.shape),
            np.prod(env.action_space.shape),
            hidden_layers=params["Layers"]
        )

        # ì´ ì‹¤í—˜ì€ "í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ 1ê°œë§Œ" ì“°ë‹ˆê¹Œ num_tasks=1
        agent = MetaGaussianAgent(
            mlp=mlp,
            num_tasks=1,
            learn_std=params["learn_std"]
        )

        meta_algo = ProMP(
            env=env,
            max_path_length=params["max_path_length"],
            agent=agent,
            alpha=params["alpha"],
            beta=params["beta"],
            baseline=LinearFeatureBaseline(),
            tensor_log=params["tensor_log"],
            inner_grad_steps=params["num_inner_grad"],
            num_tasks=1,  # ì—¬ê¸°ë„ 1
            outer_iters=params["outer_iters"],
            parallel=params["parallel"],
            rollout_per_task=params["rollout_per_task"],
            clip_eps=params["clip_eps"],
            device=params["device"]
        )

        # 3) pre-trained ëª¨ë¸ ë¡œë“œ
        meta_algo.load_state_dict(torch.load("preTrainModel/saved_model", map_location="cpu")) # ì–˜ë¥¼ ì´ìš©í•´ì„œ ì¤‘ê°„ì— ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ


        # 4) ì´ ì‹œë‚˜ë¦¬ì˜¤ìš© ì½œë°± ìƒì„± (ë§ˆì§€ë§‰ 15ê°œ í‰ê· )
        cb = ScenarioEvalCallback(
            window=15,
            scenario_idx=idx,
            scenario=scenario,
            results_list=all_results
        )

        # 5) 1000 epoch íŒŒì¸íŠœë‹
        meta_algo.learn(epochs=params["epochs"], callback=cb)

        # í•„ìš”í•˜ë©´ ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ëª¨ë¸ë„ ë”°ë¡œ ì €ì¥ ê°€ëŠ¥
        scenario_model_path = os.path.join(SAVED_MODEL_PATH, f"scenario_{idx}_model.pt")
        torch.save(meta_algo.state_dict(), scenario_model_path)

        meta_algo.close()
        print(all_results)

    # ===== ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ CSVë¡œ ì €ì¥ =====
    results_df = pd.DataFrame(all_results)
    results_path = os.path.join(SAVED_MODEL_PATH, "scenario_finetune_results.csv")
    results_df.to_csv(results_path, index=False)
    print(f"\n[Done] ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ë¥¼ {results_path} ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    
    params = {
        "Layers":[64, 64], # layers of Network
        "rollout_per_task": 20,
        "num_task": 1, # Number of tasks
        "max_path_length": SIM_TIME,
        "tensor_log": TENSORFLOW_LOGS,
        "alpha": 0.002,
        "beta": 0.0005,
        "outer_iters": 5, # number of ProMp steps without re-sampling
        "clip_eps": 0.3, # clip range for ProMP(outer) update
        "num_inner_grad": 1,
        "epochs": 1000,
        "discount": 0.99,
        "gae_lambda": 1,
        "parallel": False,
        "learn_std": True,
        "device":torch.device("cpu")
    }
    
    main(params)
   